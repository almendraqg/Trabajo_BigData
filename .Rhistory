library(tidyverse)
library(rvest)
library(stringr)
library(rebus)
library(lubridate)
library(tm)
datos <- data.frame(
"productos"="A",
"precios"="B" ,
"precios_oferta"="C"
)
n=1
for (i in 1:n) {
x="https://mashini.cl/collections/ropa-de-cama?page="
y=as.character(n)
url =  paste0(x,y)
url <- read_html(url)
productos <- html_text(html_nodes(url, ".title"))
productos = gsub("[[:cntrl:]]", "", productos)
productos  <- tolower(productos )
productos <- stripWhitespace(productos)
precios <- html_text(html_nodes(url, ".money"))
precios <- precios[-c(1)]
seq <- seq(1, length(precios), by=2)
seq2 <- seq(2, length(precios), by=2)
precios_oferta <- 0
precios_oferta <- precios[-c(seq)]
precios <- precios[-c(seq2)]
data <- cbind(productos,precios,precios_oferta)
datos <- rbind(datos,data)
Sys.sleep(2)
}
View(datos)
library(tidyverse)
mydata <- starwars
mydata
class(mydata)
## Ordinary least squares(linear model)
ols1 <- lm(mass ~ height, data = mydata)
ols1
summary(ols1)
summary(ols1)$coefficients
install.packages("broom")    #to get "tidy" regression coefficients
install.packages("broom")
library(broom)
tidy(ols1, conf.int = TRUE)
glance(ols1)
## Plotting the data and the fitted regression line
plot(mass ~ height,
data = mydata,
main = "Scatterplot of mass and height",
xlab = "Mass",
ylab = "Height")
abline(ols1)    #We add the estimated regression line
## Regression on subsetted data
qplot(height, mass, data=mydata)  #we observe an outlier
filter(mydata, mass>1000)   #we figure out that Jabba is the outlier, let's remove it
mydata2 <-
starwars %>%
filter(name != "Jabba Desilijic Tiure")   #the outlier is removed from the data
library(dplyr)
mydata2 <-
starwars %>%
filter(name != "Jabba Desilijic Tiure")   #the outlier is removed from the data
mydata2
ols2 <- lm(mass ~ height, data = mydata2)
summary(ols2)
install.packages("estimatr")
library(estimatr)
ols2_robust = lm_robust(mass ~ height, data = mydata2)
summary(ols2_robust)
## Dummy variables and interaction terms
humans <-        #use a subsample of the starwars data that comprises only the human characters
starwars %>%
filter(species=="Human")
humans
ols_dum <- lm(mass ~ height + gender, data = humans)
summary(ols_dum)
ols_dumint = lm(mass ~ gender * height, data = humans)
summary(ols_dumint)
### Presenting the regression results with "stargazer"
install.packages("stargazer")   #useful package especially if you are using Latex
library(stargazer)
stargazer(ols1, title="Results", align=TRUE)  #default type is Latex
stargazer(ols1, ols2, title="Results", align=TRUE, type="text")
### Presenting the regression results with "modelsummary"
install.packages("modelsummary")
install.packages("flextable")   #for word and powerpoint outputs
library(modelsummary)
library(flextable)
modelsummary(list(ols1, ols2), output="Table 1.docx")
install.packages("AER")    #AER for "Applied Econometrics with R"
library(AER)
data("Journals", package = "AER")   #built-in data for Journals
class(Journals)
## Presenting the descriptive statistics with "vtable"
install.packages("vtable")
library(vtable)
sumtable(Journals)
## It is always a good idea to graph and observe the data first
plot(log(subs) ~ log(citeprice), data = journals)
library(vtable)
sumtable(Journals)
sumtable(c("subs", "price"), data=Journals)
journals <- Journals[, c("subs", "price")]
journals$citeprice <- Journals$price/Journals$citations
summary(journals)
sumtable(c("subs", "price", "citeprice"), data=journals)
## It is always a good idea to graph and observe the data first
plot(log(subs) ~ log(citeprice), data = journals)
## Linear regression using ordinary least squares (OLS)
jour_lm <- lm(log(subs) ~ log(citeprice), data = journals)
class(jour_lm)
names(jour_lm)
## Useful functions for fitted linear models
print(jour_lm)        #simple printed display
summary(jour_lm)      #standard regression output
coef(jour_lm)         #(or coefficients()) extracting the regression coefficients
residuals(jour_lm)    #(or resid()) extracting residuals
fitted(jour_lm)       #(or fitted.values()) extracting fitted values
anova(jour_lm)        #comparison of nested models
confint(jour_lm, level=0.95)    #confidence intervals for the regression coefficients
deviance(jour_lm)     #residual sum of squares
vcov(jour_lm)         #(estimated) variance-covariance matrix
AIC(jour_lm)          #information criteria including AIC, BIC/SBC (assuming normally distributed errors)
set.seed(123)    #For reproducible outcomes
sample(1:10)                  # Permutation
sample(1:10, 4)               # Sample 4 random entries from integers 1 to 10
sample(1:10, replace=TRUE)    # Sampling with replacement
sample(1:6, 3, replace = T)   # Rolling a dice three times
mean(sample(1:6, 10000, replace = T))  # compute the sample mean of 10000 dice rolls
var(sample(1:6, 10000, replace = T))   # compute the sample variance of 10000 dice rolls
sample(c("H", "T"), 1)  #simulate coin tossing with outcomes heads and tails
# P(X=5) five heads in ten tosses of a fair coin
dbinom(x = 5, size = 10, prob = 0.5)
# compute P(4 <= k <= 7) using 'dbinom()'
sum(dbinom(x = 4:7, size = 10, prob = 0.5))
# compute P(4 <= k <= 7) using 'pbinom()'
pbinom(size = 10, prob = 0.5, q = 7) - pbinom(size = 10, prob = 0.5, q = 3)
# set up vector of possible outcomes to see the pdf
k <- 0:10
k
# assign the probabilities
probability <- dbinom(x = k,
size = 10,
prob = 0.5)
# plot the outcomes against their probabilities
plot(x = k,
y = probability,
main = "Probability Distribution Function")
# compute cumulative probabilities
prob <- pbinom(q = k,
size = 10,
prob = 0.5)
# plot the cumulative probabilities
plot(x = k,
y = prob,
main = "Cumulative Distribution Function")
dpois(x=3, lambda=0.5)  # P(x=3) three arrivals at an ATM in the next minute, where the average number of arrivals per minute is 0.5
ppois(q=3, lambda=0.5)  # P(x<=3) at most three arrivals at an ATM in the next minute, where the average number of arrivals per minute is 0.5
rpois(n=3, lambda=0.5)  # Generating 3 Poisson random variables for the same experiment
# define functions
f <- function(x) 3 / x^4     #let's say the function is defined at x>1
# find probability by computing area under the density curve of f
area <- integrate(f, lower = 1, upper = Inf)$value
area
# compute E(X)
g <- function(x) x * f(x)    #formula for E(x)
EX <- integrate(g, lower = 1, upper = Inf)$value
EX
# compute Var(X) (Remember the varaince formula Var(x)=E(x^2)-[E(x)]^2)
h <- function(x) x^2 * f(x)
VarX <- integrate(h, lower = 1, upper = Inf)$value - EX^2
VarX
runif(5)     #generate 5 uniform random numbers on the interval [0,1]
runif(10, min=-3, max=-1)  #generate 10 uniform random numbers on the interval [-3,-1]
pnorm(84, mean=72, sd=15.2, lower.tail=FALSE)  #P(x>=84) where test scores of a college entrance exam fits a normal distribution with mean test score 72 and the standard deviation 15.2
# plot the standard normal N(0,1) PDF
curve(dnorm(x),
xlim = c(-3.5, 3.5),
ylab = "Density",
main = "Standard Normal Density Function")
# plot the standard normal CDF
curve(pnorm(x),
xlim = c(-3.5, 3.5),
ylab = "Probability",
main = "Standard Normal Cumulative Distribution Function")
# supppose you want to find the probability P(3<=Y<=4) for Y~N(5,25)
pnorm(4, mean = 5, sd = 5) - pnorm(3, mean = 5, sd = 5)
# plot the PDF
curve(dchisq(x, df = 3),
xlim = c(0, 10),
ylim = c(0, 1),
col = "blue",
ylab = "",
main = "PDF and CDF of Chi-Squared Distribution, dof = 3")
# add the CDF to the plot
curve(pchisq(x, df = 3),
xlim = c(0, 10),
add = TRUE,
col = "red")
# add a legend to the plot
legend("topleft",
c("PDF", "CDF"),
col = c("blue", "red"),
lty = c(1, 1))
# plot the t density for M=2
curve(dt(x, df = 2),
xlim = c(-4, 4),
ylim = c(0, 0.5),
col = 1,        #col=1 is black
xlab="x",
ylab="Density",
main = "Comparing Densities of t and Z Distributions")
# plot the standard normal density
curve(dnorm(x),
xlim = c(-4, 4),
add = TRUE,
col = 2)        #col=2 is red
### One-Sample t-test:
daily.intake <- c(5260,5470,5640,6180,6390,6515,
+ 6805,7515,7515,8230,8770)    #in kJ for 11 women
mean(daily.intake)
sd(daily.intake)
#test whether this distribution might have mean = 7725
t.test(daily.intake,mu=7725)
#you can change the confidence level
t.test(daily.intake,mu=7725, conf.level=0.99)
#you can also conduct a one-sided test
t.test(daily.intake,mu=7725, alternative="greater")
t.test(daily.intake,mu=7725, alternative= "less")
### Two-Sample t-test:
library(ISwR)
install.packages("ISwR")
energy
?energy
attach(energy)
#Welch Two-Sample t-test: You do not assume that the variance is the same in the two groups
t.test(expend~stature)
#Usual t-test: Assume that the variances are the same in the two groups
t.test(expend~stature, var.equal=T)
### Comparison of Variances with F-test:
var.test(expend~stature)
### The paired t-test: (Matched Pairs)
intake
?intake
data(intake)
attach(intake)
data(CASchools)
# define variables
CASchools$STR <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math)/2
# compute correlations
cor(CASchools$STR, CASchools$score)
cor(CASchools$STR, CASchools$english)
# estimate both regression models
simp.mod <- lm(score ~ STR, data = CASchools)
simp.mod
mult.mod <- lm(score ~ STR + english, data = CASchools)
mult.mod
summary(mult.mod)
summary(mult.mod)$coef
# define the components
n <- nrow(CASchools)                            # number of observations (rows)
k <- 2                                          # number of regressors
y_mean <- mean(CASchools$score)                 # mean of avg. test-scores
SSR <- sum(residuals(mult.mod)^2)               # sum of squared residuals
TSS <- sum((CASchools$score - y_mean )^2)       # total sum of squares
ESS <- sum((fitted(mult.mod) - y_mean)^2)       # explained sum of squares
# compute the measures
SER <- sqrt(1/(n-k-1) * SSR)                    # standard error of the regression
Rsq <- 1 - (SSR / TSS)                          # R^2
adj_Rsq <- 1 - (n-1)/(n-k-1) * SSR/TSS          # adj. R^2
# print the measures
c("SER" = SER, "R2" = Rsq, "Adj.R2" = adj_Rsq)
# estimate a set of models with alternative independent variables
spec1 <- lm(score ~ computer, data = CASchools)
spec2 <- lm(score ~ computer + english, data = CASchools)
spec3 <- lm(score ~ computer + english + lunch, data = CASchools)
spec4 <- lm(score ~ computer + english + calworks, data = CASchools)
spec5 <- lm(score ~ computer + english + lunch + calworks, data = CASchools)
# generate a table for comparison using stargazer package
library(stargazer)
stargazer(spec1, spec2, spec3, spec4, spec5,
digits = 3,
header = F,
column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)"),
type="text")
View(data)
